{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a6b9a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1340d",
   "metadata": {},
   "source": [
    "## data coming from wikipedia that we loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1add6d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C://Users/zohairk/OneDrive/Desktop/personalrepo/Modules/module8 ETL/wikipedia-movies.json'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the Json file in pandas\n",
    "file_dir = 'C://Users/zohairk/OneDrive/Desktop/personalrepo/Modules/module8 ETL/'\n",
    "# open the file \n",
    "f'{file_dir}wikipedia-movies.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "99a40b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the raw JSON into a list of dictionaries, we will use the load() method.\n",
    "with open(f'{file_dir}/wikipedia-movies.json', mode='r') as file:\n",
    "    wiki_movies_raw = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b05ce",
   "metadata": {},
   "source": [
    "Here, wiki_movies_raw is now a list of dicts. Before we take a look at the data, we should check how many records were pulled in. We can use the len() function (see below), which returns 7,311 records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c3761",
   "metadata": {},
   "source": [
    "## load in the Kaggle data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7f04cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_metadata = pd.read_csv(f'{file_dir}movies_metadata.csv', low_memory=False)\n",
    "ratings = pd.read_csv(f'{file_dir}ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0e160",
   "metadata": {},
   "source": [
    "### Deliverable 4 : finished the Transform step in ETL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1ea45",
   "metadata": {},
   "source": [
    "#### Last step of ETL is loading our tables into SQL. [section 8.5.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "587ede32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after creating \"movie_data\" database in pgAdmin, the following code is done to\n",
    "# import create_engine from the sqlalchemy module\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e746b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Database Engine for pandas and sql communication\n",
    "from config import db_password\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f52698d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our local server, the connection string will be as follows:\n",
    "\n",
    "db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "07882f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database engine\n",
    "engine = create_engine(db_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "633afe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in c:\\users\\zohairk\\anaconda3\\envs\\pythondata\\lib\\site-packages (2.9.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26356632",
   "metadata": {},
   "source": [
    "###### Import the Movie Data\n",
    "To save the movies_df DataFrame to a SQL table, we only have to specify the name of the table and the engine in the to_sql() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7999bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "movies_df.to_sql(name='movies', con=engine)\n",
    "# In pgAdmin, confirm that the table imported correctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "82aa8c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...Done.\n",
      "importing rows 1000000 to 2000000...Done.\n",
      "importing rows 2000000 to 3000000...Done.\n",
      "importing rows 3000000 to 4000000...Done.\n",
      "importing rows 4000000 to 5000000...Done.\n",
      "importing rows 5000000 to 6000000...Done.\n",
      "importing rows 6000000 to 7000000...Done.\n",
      "importing rows 7000000 to 8000000...Done.\n",
      "importing rows 8000000 to 9000000...Done.\n",
      "importing rows 9000000 to 10000000...Done.\n",
      "importing rows 10000000 to 11000000...Done.\n",
      "importing rows 11000000 to 12000000...Done.\n",
      "importing rows 12000000 to 13000000...Done.\n",
      "importing rows 13000000 to 14000000...Done.\n",
      "importing rows 14000000 to 15000000...Done.\n",
      "importing rows 15000000 to 16000000...Done.\n",
      "importing rows 16000000 to 17000000...Done.\n",
      "importing rows 17000000 to 18000000...Done.\n",
      "importing rows 18000000 to 19000000...Done.\n",
      "importing rows 19000000 to 20000000...Done.\n",
      "importing rows 20000000 to 21000000...Done.\n",
      "importing rows 21000000 to 22000000...Done.\n",
      "importing rows 22000000 to 23000000...Done.\n",
      "importing rows 23000000 to 24000000...Done.\n",
      "importing rows 24000000 to 25000000...Done.\n",
      "importing rows 25000000 to 26000000...Done.\n",
      "importing rows 26000000 to 26024289...Done.\n"
     ]
    }
   ],
   "source": [
    "# Import the Ratings Data; thi will be done in chunk size as it is very large file. \n",
    "# Do not run this yet!\n",
    "# create a variable for the number of rows imported\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv(f'{file_dir}ratings.csv', chunksize=1000000):\n",
    "\n",
    "    # print out the range of rows that are being imported\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "\n",
    "    data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "    # increment the number of rows imported by the size of 'data'\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    # print that the rows have finished importing\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a3675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
